# Supervise (main.py)
mkdir checkpoint/language; mkdir mappings/language;
PYTHONPATH=. python main.py --schema-path data/craigslist-schema.json --train-examples-paths data/train-luis-post-new.json --test-examples-paths data/dev-luis-post-new.json \
--price-tracker data/price_tracker.pkl \
--model lf2lf \
--model-path checkpoint/language --mappings mappings/language \
--word-vec-size 300 --pretrained-wordvec '' '' \
--rnn-size 300 --rnn-type RNN --global-attention multibank_general \
--num-context 2 --stateful \
--batch-size 128  --optim adagrad --learning-rate 0.01 \
--epochs 15 --report-every 500 \
--cache cache/language --ignore-cache \
--verbose  --state-length 4 --dia-num 20 --use-utterance --hidden-size 64

# tom_itentity_test
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/language/model_best.pt checkpoint/language/model_best.pt \
--model-path checkpoint/a2c --mappings mappings/language \
--optim adam --rnn-type RNN --rnn-size 300 --max-grad-norm -1 \
--agents pt-neural pt-neural-r \
--report-every 50 --max-turns 20 --num-dialogues 2560 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 --epochs 2000 --use-utterance \
--model lf2lf --model-type a2c --identity-test --load-sample cache/identity/data.pkl \
--learning-rate 0.001 --name identity_lr0.001_size128_adam2k --tom-hidden-size 128 --hidden-depth 1 --gpu 0

# With utterance as input:
* --use-utterance
* --bert-model-path [path] (optional, if pretrain model saved)

# Reinforcement Learning(mult_rl.py / reinforce.py)
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/language/model_best.pt checkpoint/language/model_best.pt \
--model-path checkpoint/a2c --mappings mappings/language \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural-r \
--report-every 50 --max-turns 20 --num-dialogues 10000 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type a2c --name a2c

PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/a2c_decay2/model_reward0.39_e55.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/test --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural-r \
--report-every 50 --max-turns 20 --num-dialogues 10000 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type a2c --name test --gpuid 0 --num-cpus 5 --price-strategy decay \
--warmup-epochs 25 --only-run

# reinforce
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl/model_best.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/reinforce --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural-r \
--report-every 50 --max-turns 20 --num-dialogues 10000 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type reinforce --name reinforce --gpuid 0 --num-cpus 5 --price-strategy decay \
--warmup-epochs 25

PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl/model_best.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/a2c --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural \
--report-every 50 --max-turns 20 --num-dialogues 10000 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type a2c --name a2c --gpuid 0 --num-cpus 5 \
--warmup-epochs 25


# Theory of Mind
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl/model_best.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/tom_decay6 --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents tom pt-neural-r \
--report-every 50 --max-turns 20 --num-dialogues 10000 \
--sample --temperature 0.5 --max-length 20 --reward margin \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type tom --name tom_decay6 --price-strategy decay --tom-type expectation --gpuid 0 \
--warmup-epochs 25 --num-cpus 13

# --agents pt-neural/pt-neural-r/tom
# --model-type reinforce/a2c/tom
# --train-mode normal/fix_value/fix_policy/none
# --price-strategy low/high/decay/neural
# --tom-type cooperative/competitive/expectation
# --temperature 0.5

# --num-cpus 1
# --gpuid 0


# Old Commands(abandoned)

PYTHONPATH=. python tom.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/normal_lf2lf-balance/model_best.pt checkpoint/normal_lf2lf/model_best.pt \
--critic-path checkpoint/normal_lf2lf-ac \
--optim adagrad --learning-rate 0.05 \
--agents tom pt-neural \
--report-every 100 --max-turns 20 --num-dialogues 5000 \
--sample --temperature 0.5 --max-length 20 --reward balance


mkdir checkpoint/lf2lf-ac;
PYTHONPATH=. python critic.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/lf2lf-balance/model_best.pt checkpoint/lf2lf/model_best.pt \
--model-path checkpoint/lf2lf-ac \
--optim adagrad --learning-rate 0.08 \
--agents ac ac \
--report-every 100 --max-turns 20 --num-dialogues 5000 \
--sample --temperature 0.5 --max-length 20 --reward balance

mkdir -p mappings/lf2lf;
mkdir -p cache/lf2lf;
mkdir -p checkpoint/lf2lf;
PYTHONPATH=. python main.py --schema-path data/craigslist-schema.json --train-examples-paths data/train-luis-post.json --test-examples-paths data/dev-luis-post.json \
--price-tracker data/price_tracker.pkl \
--model lf2lf \
--model-path checkpoint/lf2lf --mappings mappings/lf2lf \
--word-vec-size 300 --pretrained-wordvec '' '' \
--rnn-size 300 --rnn-type LSTM --global-attention multibank_general \
--num-context 2 --stateful \
--batch-size 128 --gpuid 0 --optim adagrad --learning-rate 0.01 \
--epochs 15 --report-every 500 \
--cache cache/lf2lf --ignore-cache \
--verbose


mkdir checkpoint/lf2lf-balance;
PYTHONPATH=. python reinforce.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/lf2lf/model_best.pt checkpoint/lf2lf/model_best.pt \
--model-path checkpoint/lf2lf-balance \
--optim adagrad --learning-rate 0.001 \
--agents pt-neural pt-neural \
--report-every 500 --max-turns 20 --num-dialogues 5000 \
--sample --temperature 0.5 --max-length 20 --reward balance


Supervise Learning:
mkdir checkpoint/sl; mkdir mappings/sl;PYTHONPATH=. python main.py --schema-path data/craigslist-schema.json --train-examples-paths data/train-luis-clean.json --test-examples-paths data/dev-luis-clean.json \
--price-tracker data/price_tracker.pkl \
--model lf2lf \
--model-path checkpoint/sl2 --mappings mappings/sl2 \
--word-vec-size 20 --pretrained-wordvec '' '' \
--hidden-size 64 --rnn-type LSTM --global-attention multibank_general \
--num-context 2 --stateful \
--batch-size 128 --gpuid 0 --optim adagrad --learning-rate 0.01 \
--epochs 20 --report-every 500 \
--cache cache/sl2 --ignore-cache --gpuid 0 \
--dia-num 20 --state-length 4


Reinforce:
mkdir checkpoint/rl2;                    
PYTHONPATH=. python reinforce.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl2/model_best.pt checkpoint/sl2/model_best.pt \
--model-path checkpoint/rl2 \
--optim adagrad --learning-rate 0.001 \
--agents pt-neural pt-neural \
--report-every 100 --max-turns 20 --num-dialogues 5000 \
--sample --temperature 0.5 --max-length 20 --reward balance \
--dia-num 20 --state-length 4



Critic:
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl/model_best.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/test --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural \
--report-every 50 --max-turns 20 --num-dialogues 500 \
--sample --temperature 0.5 --max-length 20 --reward balance \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type a2c --num-cpus 2

A2c:
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/sl/model_best.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/test --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents pt-neural pt-neural \
--report-every 50 --max-turns 20 --num-dialogues 500 \
--sample --temperature 0.5 --max-length 20 --reward balance \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type a2c --num-cpus 2


Tom:
PYTHONPATH=. python multi_rl.py --schema-path data/craigslist-schema.json \     
--scenarios-path data/train-scenarios.json \
--valid-scenarios-path data/dev-scenarios.json \
--price-tracker data/price_tracker.pkl \
--agent-checkpoints checkpoint/a2c3/model_reward1.17_e18.pt checkpoint/sl/model_best.pt \
--model-path checkpoint/tom --mappings mappings/sl \
--optim adagrad --learning-rate 0.01 \
--agents tom pt-neural \      
--report-every 50 --max-turns 20 --num-dialogues 5000 \
--sample --temperature 0.5 --max-length 20 --reward balance \
--dia-num 20 --state-length 4 \
--model lf2lf --model-type tom  --name tom_real --num-cpus 6 --gpuid 0


